Training begins with the supervised policy learned in Phase I. At a basic level, this policy is simply played against itself iteratively, updating policy weights according to whether a game was won or lost. One copy of the policy is continually updated (and hopefully improved), and periodically frozen copies of itself are added to the pool of opponents. Of course, when starting out, the pool has size one, but grows over time.

Games are played in "mini-batches". In each mini-batch, the policy chooses an opponent at random from the pool, to play a set of games against. During play, state-action pairs and winners are recorded for later training. After a mini-batch has been played out, network weights for the policy are updated over all training pairs. In Google's implementation, they run mini-batches of size 128, in parallel. They play 500 mini-batches per iteration (i.e. the policy sees 500 opponents), and add a copy of the policy to the opponent pool after each iteration. Google's system was trained on 10,000 mini-batches. During the second pass through RL learning (Phase IV), they used the value network as a baseline for each training example, to reduce variance (see paper).

Here's how our implementation works.

```python
# Set SGD and compile
sgd = SGD(lr=args.learning_rate)
player.policy.model.compile(loss='binary_crossentropy', optimizer=sgd)
for i_iter in xrange(args.iterations):
	# Train mini-batches
	for i_batch in xrange(args.save_every):
		# Randomly choose opponent from pool
		opp = np.random.choice(opponents)
		# Make training pairs and do RL
		X_list, y_list, winners = make_training_pairs(
			player, opp, features, args.game_batch_size)
		player = train_batch(player, X_list, y_list, winners, args.learning_rate)
```

So, after initializing our Keras SGD object and compiling the policy's neural network model, we loop through the numbers of iterations and batches specified at the command line. Note the `player` object is an instance of `ProbabilisticPolicyPlayer`, which is just another layer of abstraction that offers a `get_move` function that returns a move when given a board state. `X_list` and `y_list` are the training states and actions, respectively. Now, let's look at what happens inside `make_training_pairs`.

```python
while True:
	# Cache states before moves
	states_prev = [st.copy() for st in states]
	# Get moves (batch)
	moves_black = player1.get_moves(states)
	# Do moves (black)
	states, X_list, y_list = do_move(states, states_prev, moves_black,
					 X_list, y_list, player_color)
	# Do moves (white)
	moves_white = player2.get_moves(states)
	states, X_list, y_list = do_move(states, states_prev, moves_white,
		                         X_list, y_list, player_color)
	# If all games have ended, we're done. Get winners.
	done = [st.is_end_of_game or st.turns_played >= 500 for st in states]
	if all(done):
		break
winners = [st.get_winner() for st in states]
```

The while loop ensures that all games are played out until the end. `states` is a list of GameState objects, which keep track of each of the games (current player, turns played, board state, etc.). `X_list` and `y_list` are nested lists, where each sub-list represents training items from one of the games being played in parallel. For each iteration of the while loop, the lists corresponding to each game get appended (in `do_move`) as long as that game hasn't already ended. (Here, I've also added an arbitrary cap of 500 turns to all games, because some might get stuck in ko loops.) We only record an (X, y) training pair if it's the policy's move and the policy didn't choose to pass. Before the games start, the policy is randomly assigned to be either white or black. So `player1` may be the policy and `player2` the opponent, or vice versa in any given mini-batch. Here's what happens in `do_move`.

```python
for st, st_prev, mv, X, y in zip(states, states_prev, moves, X_list,
				 y_list):
	if not st.is_end_of_game:
		# Only do more moves if not end of game already
		st.do_move(mv)
		if st.current_player != player_color and mv is not go.PASS_MOVE:
			# Convert move to one-hot
			state_1hot = preprocessor.state_to_tensor(st_prev)
			move_1hot = np.zeros(bsize_flat)
			move_1hot[flatten_idx(mv, bsize)] = 1
			X.append(state_1hot)
			y.append(move_1hot)
```

Here we're converting the board states and move tuples to 1-hot encoding, which is just the format needed for our neural network model. Then, we append those to training items to `X_list` and `y_list`. Later on, the sub-lists in X_list and y_list will be concatenated into a single numpy array per game in the mini-batch. Now, let's look at how `train_batch` works.

```python
for X, y, winner in zip(X_list, y_list, winners):
	# Update weights in + direction if player won, and - direction if player lost.
	# Setting learning rate negative is hack for negative weights update.
	if winner == -1:
		player.policy.model.optimizer.lr.set_value(-lr)
	else:
		player.policy.model.optimizer.lr.set_value(lr)
	player.policy.model.fit(X, y, nb_epoch=1, batch_size=len(X))
```

Note that, even though we're doing reinforcement learning, we're able to make use of Keras's backpropagation (`model.fit`) to update the weights, through the use of a simple hack: we can change the direction the weights are updated by setting the learning rate negative. So we set the learning rate positive if we won a game, since we want to reward this behavior, and negative if we lost, to penalize. All moves in the game leading to the reward outcome are weighted equally.

Those are the basics. You can refer to our code base for more implementation details. But remember that the code is a work in progress, so the wiki may be slow to reflect any changes.