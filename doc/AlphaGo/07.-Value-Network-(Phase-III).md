The value network estimates, in reinforcement learning terms, the value of each position. That is, if I see a given board state, and played an optimal policy until the end of the game, what would be my expected reward (i.e. winning/losing)? The value network training has two phases: i) games of self-play to generate training pairs, ii) update weights using those training pairs. In broad strokes, the generation part takes the policy we learned from RL (Phase II) and plays it against itself (DeepMind played 30 million games), then in training it's basically the same thing we saw in Phase II. But there are a couple subtle nuances. First, one move (__U__) during each game is chosen to be played at random, where __U__ is sampled uniformly from 1 to 450. The action is drawn from a uniform random distribution over all board positions (excluding illegal moves). All turns prior to _U_, both players use the SL, and afterwards they are replaced with the RL. Also, only one training tuple is collected from each game. The tuple consists of the board state just after the random move (__s<sub>U+1</sub>__) and the eventual winner (white or black). Additionally, the color of player who played __a<sub>U+1</sub>__ is recorded for use in input to the network during training. Presumably, only taking one move per game avoids the overfitting that happens if every move is taken from each game. Notice the action is not included, because this is the value of the board position, alone.

First, let's look at the script for generating the 30 million training examples.

```python
X_list = []
winners_list = []
colors_list = []
for n in xrange(n_training_pairs / batch_size):
    X, winners, colors = play_batch(player_RL, player_SL, batch_size, features)
    X_list.append(X)
    winners_list.extend(winners)
    colors_list.extend(colors)
# Concatenate over batches to make one HDF5 file
X = np.concatenate(X_list, axis=0)
save(out_pth, X, winners_list, colors_list)
```

`X_list` is a list of board states to train on (one per game), and `winners_list` and `colors_list` are the other training elements as described above. As is Phase II, we play games in batches (for GPU efficiency). So we loop over batches until we've played the desired number of games (i.e. `n_training_pairs`). When we've finished looping, we save all the data out to HDF5, which is an efficient standard for storing large files. Now, let's examine `play_batch`.

```python
player = player_SL
states = [GameState() for i in xrange(batch_size)]
# Randomly choose turn to play uniform random. Move prior will be from SL
# policy. Moves after will be from RL policy.
i_rand_move = np.random.choice(range(450))
while True:
    # Do moves (black)
    if turn == i_rand_move:
        # Make random move, then switch from SL to RL policy
        X_list, colors, states, player = do_rand_move(states, player,
                                                      player_RL)
    else:
        # Get moves (batch)
        moves_black = player.get_moves(states)
        # Do moves (black)
        states = do_move(states, moves_black)
    # Do moves (white)
    if turn == i_rand_move:
        # Make random move, then switch from SL to RL policy
        X_list, colors, states, player = do_rand_move(states, player,
                                                      player_RL)
    else:
        moves_white = player.get_moves(states)
        states = do_move(states, moves_white)
    # If all games have ended, we're done. Get winners.
    done = [st.is_end_of_game or st.turns_played >= 500 for st in states]
    if all(done):
        break
``` 

This should look similar to Phase II. The major difference is that there's some extra logic to check each move whether the current turn is the one we've randomly chosen to be off-policy (uniform random). `do_rand_move` does the random move, then returns the training tuple data and reassigns the `player` to the `player_RL`.

Great, so now we have more training examples we could ever want. But how do we train? First, a bit about the network architecture. The value network is similar to the policy network with some subtle differences. Both networks have 48 feature planes as input, but the value has an additional binary feature plane describing the current color to play. There are some other differences, which I won't go into detail about here, but are detailed in the paper. The most notable difference is that the last layer of the value network is a single tanh unit (because it's trying to guess whether the game outcome was -1 or 1), whereas the policy network used softmax (because it was classifying). Network implementation can be found in AlphaGo/models/value.py.

Finally, the training is identical to the supervised (Phase I), except that data labels are just 1 or -1 (as opposed to flattened board states). The code in `reinforcement_value_trainer.py` is almost identical to `supervised_policy_trainer.py`, with just a few tweaks, like changing the loss to mean squared error.