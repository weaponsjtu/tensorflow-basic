## Welcome to the AlphaGo Replication Wiki

Here you can learn more about how this project is structured, and a little about how DeepMind's AlphaGo itself works.

For details, see the list of pages in the sidebar on the right.

## How AlphaGo Works

[[https://www.deepmind.com/alpha-go.html]]

DeepMind's AlphaGo is a combination of [Monte Carlo Tree Search](https://en.wikipedia.org/wiki/Monte_Carlo_tree_search) (MCTS) with [Convolutional Neural Networks](https://en.wikipedia.org/wiki/Convolutional_neural_network). There are _three_ networks actively contributing to finding a move:

* the __policy__ network guesses where an expert would play. Think of this network as memorizing patterns that it has seen others play but without any sense of the value of playing there.
* the __value__ network estimates the probability of winning from the current position. It is perhaps analogous to an expert's 'intuition' of preferring one position to another.
* a second __fast policy__ network that is used to simulate playouts to the end of the game. It is not a particularly strong player, but is orders of magnitude faster than the first policy network.

Together, these can be used to choose a move - by playing out a short series of reasonable moves then evaluating the results, then picking the best option. This is what AlphaGo's MCTS does - it simply follows the search tree in the most promising directions, where how 'promising' a branch is is a combination of expert _policy_ and estimated _value_.

The fast policy network is not what you would think of as a typical neural network - it is more like [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) to choose the next move. This rollout policy is not nearly as good as the deeply-learned policy, but it is much much faster. Since the good policy function is so slow, it is only used to search ahead about 20 moves. Normally the value function alone could evaluate how good the board looks after those 20 moves, but AlphaGo's value function is augmented using this fast rollout all the way to the end of the game and simply seeing who wins.

The networks here aren't just processing stone positions or images of boards. A given board position is _preprocessed_ into features that the neural network can make better use of. Part of the success of AlphaGo is attributable to carefully choosing which features to use. More complex features can be informative but at the expense of having to compute them for each position in the search tree.

The other big development contributing to AlphaGo's success is [[how these networks are trained using a combination of supervised and reinforcement learning|04. neural networks and training]].

<!-- * parallelize tree search
* optimize the combined tree search + feature processing + neural network player.
	* Goal:  -->

<!-- **ongoing development**

* server and UI
	* implement Go Text Protocol
* testing against other players (e.g. michi and pachi)
* further search optimizations (e.g. early stopping when confident)
* ports to other langauges, possibly only requiring the player itself (as opposed to implementing the full training pipeline) -->