Training begins by teaching a convolutional network to simply predict where an expert would move. See the [[data|03.-Data]] page regarding where 'expert' data comes from.

Each board position and its subsequent move from the database form a training pair. Additional pairs are constructed using rotations and reflections of the board. SGD and backpropagation are used to maximize the probability of selecting the given action.

DeepMind reported achieving 57% accuracy predicting moves in a test set after this phase (Silver et al. page 2).

Training of the fast rollout policy uses the same dataset and therefore will share much of the same training code. The features and configuration of this network are very different than those of the deep policy network.

#### Supervised training script

To see what arguments are available, use

    python -m AlphaGo.training.supervised_policy_trainer --help

This script is given a model file (a json specifying the policy network's architecture) and a HDF5 file and calls Keras' `fit_generator` on data extracted by shuffling the given dataset. Each time this script is run, it should be pointed to a different 'output' directory. Metadata and model weights each epoch are saved in this directory.

The output directory will be populated with:

* metadata.json - a json file that keeps a record of which model is being trained, on what dataset, and the train/val accuracy on each epoch
* shuffle.npz - shuffling indices used to index into the dataset (which itself is in contiguous blocks of individual games). Training example `i` is `dataset[shuffle[i]]`
* weights.NN.hdf5 - where `NN` is the epoch number starting from `00`

To resume training from an earlier point, use the `--weights weights.##.hdf5` argument, where `##` refers the latest epoch (if you resume training from an earlier point, the metadata will be wrong). The script will look for weights in the same output directory, i.e. `os.path.join(output_dir, weights_file)`.

A model spec must exist before running this script. This can be done as follows:

```python
from AlphaGo.models.policy import CNNPolicy
arch = {'filters_per_layer': 128, 'layers': 12} # args to CNNPolicy.create_network()
features = ['board', 'ones', 'turns_since'] # must match args to game_converter
policy = CNNPolicy(features, **arch)
policy.save_model('my_model.json')
```

__Example 1: train on mini dataset created above and save results to training_results/__

    python -m AlphaGo.training.supervised_policy_trainer my_model.json debug_feature_planes.hdf5 training_results/ --epochs 5 --minibatch 32 --learning-rate 0.01
