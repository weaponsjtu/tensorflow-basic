## The training pipeline

The neural networks behind AlphaGo's success are trained in a series of steps, partially described in the main body of the [paper](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html). To really understand the details, look at the paper's Methods section.

### Phase 0: preprocessing features

As briefly described on the [[home|01. Home]] page, AlphaGo's neural networks are not processing raw board positions. Rather, they are directly being fed information like "if black plays here, how many white stones will be captured?" For a full list of features used for each network, see the Extended Data Tables in [paper](http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html).

A potential take-away from this is that convolutional neural networks are very good at recognizing (and often generalizing from) spatial patterns, but they are _not_ good at inferring arbitrary and complex rule-based relationships. Choosing which features to provide to the network is a balance of giving it good information to work with, while using features that are fast to compute and that are _usable_ by a the network.

These features are encoded as "one-hot". Take the "liberties" feature for example, which simply counts the number of liberties the group a stone is connected to has. Instead of treating the number of liberties as a scalar value, the feature given to the network is an _indicator bit_ in a binary vector, for example `10000000` if the group has one liberty, `00010000` if the group has 4 liberties, or `00000001` if the group has 8-or-more liberties. Take this miniature 7x7 board..

![black (0,0) (0,1) (0,2) (4,2) (3,3) (5,3) white (1,0) (1,1) (3,4) (4,5) (5,4)](http://i.imgur.com/V5RF4Cg.png)

The number of liberties of the group connected to each stone is

![](http://i.imgur.com/ydvVsIb.png)

There is a one-hot (i.e. indicator bit in a binary vector) encoding _at each position_, so we can visualize the resulting feature in 3D (cubes as `1`s and empty space as `0`s)

![](http://i.imgur.com/1Eo5JRG.png)

Note: in our code base, `GameState` objects know nothing about one-hot encoding. Converting from a `GameState` object to some NN inputs is handled by the `Preprocessor` class. The order of the dimensions of these features is `batch x depth x width x height` using the Theano convention. These dimensions would need to be permuted to use TensorFlow.

#### Feature conversion script

To see what arguments are available, use

    python -m AlphaGo.preprocessing.game_converter --help

This script generates an HDF5 File with a `states` dataset, an `actions` dataset, and a `file_offsets` group. The `states` dataset has size `n_data x feature_planes x board_width x board_height`, storing states converted to concatenated one-hot features at each board position. The `actions` dataset has size `n_data x 2` and simply stores the `x,y` coordinate of the move made from the corresponding state.

Typically all you do with this script is specify a list of SGF files and the particular set of features you want to process. SGF files may be specified as a directory walk, a flat lookup into a directory, or by `stdin`.

__Example 1: convert all SGFs in the tests/ directory to all available features__

    python -m AlphaGo.preprocessing.game_converter --features all --directory tests --recurse -o debug_feature_planes.hdf5

__Example 2: convert SGFs with 'Lee-Sedol' in the name, via stdin, to first 4 features planes__

    find tests -iname "*Lee-Sedol*.sgf" | python -m AlphaGo.preprocessing.game_converter --features board,ones,turns_since -o debug_feature_planes.hdf5

__Example 3: converting many SGFs in parallel__

Our [data repository](https://github.com/Rochester-NRT/RocAlphaGo.data/tree/master/scripts) contains a few scripts that we have found useful. To distribute different SGFs to multiple instances of the conversion script, we use

    find path/to/sgfs -name "*.sgf" | python skip.py 3 0 | python -m AlphaGo.preprocessing.game_converter --features board,ones,turns_since -o output/path/subsets/feature_planes0.hdf5
    find path/to/sgfs -name "*.sgf" | python skip.py 3 1 | python -m AlphaGo.preprocessing.game_converter --features board,ones,turns_since -o output/path/subsets/feature_planes1.hdf5
    find path/to/sgfs -name "*.sgf" | python skip.py 3 2 | python -m AlphaGo.preprocessing.game_converter --features board,ones,turns_since -o output/path/subsets/feature_planes2.hdf5

(This is easily done using a bash loop, for example). Next, use the `h5_concatenation_snippet.py` script to concatenate each of these HDF5 datasets into one training set:

    python h5_concatenation_snippet.py --directory output/path/ --outfile output/path/features.hdf5

### Phase 1: supervised learning of policy networks

Training begins by teaching a convolutional network to simply predict where an expert would move. See the [[data|data]] page regarding where 'expert' data comes from.

Each board position and its subsequent move from the database form a training pair. Additional pairs are constructed using rotations and reflections of the board. SGD and backpropagation are used to maximize the probability of selecting the given action.

DeepMind reported achieving 57% accuracy predicting moves in a test set after this phase (Silver et al. page 2).

Training of the fast rollout policy uses the same dataset and therefore will share much of the same training code. The features and configuration of this network are very different than those of the deep policy network.

#### Supervised training script

To see what arguments are available, use

    python -m AlphaGo.training.supervised_policy_trainer --help

This script is given a model file (a json specifying the policy network's architecture) and a HDF5 file and calls Keras' `fit_generator` on data extracted by shuffling the given dataset. Each time this script is run, it should be pointed to a different 'output' directory. Metadata and model weights each epoch are saved in this directory.

The output directory will be populated with:

* metadata.json - a json file that keeps a record of which model is being trained, on what dataset, and the train/val accuracy on each epoch
* shuffle.npz - shuffling indices used to index into the dataset (which itself is in contiguous blocks of individual games). Training example `i` is `dataset[shuffle[i]]`
* weights.XXXXX.hdf5 - where `XXXXX` is the epoch number starting from `00000`

To resume training from an earlier point, use the `--weights weights.#####.hdf5` argument, where `#####` refers the latest epoch (if you resume training from an earlier point, the metadata will be wrong). The script will look for weights in the same output directory, i.e. `os.path.join(output_dir, weights_file)`.

A model spec must exist before running this script. This can be done as follows:

```python
from AlphaGo.models.policy import CNNPolicy
arch = {'filters_per_layer': 128, 'layers': 12} # args to CNNPolicy.create_network()
features = ['board', 'ones', 'turns_since'] # Important! This must match args to game_converter
policy = CNNPolicy(features, **arch)
policy.save_model('my_model.json')
```

__Example 1: train on mini dataset created above and save results to training_results/__

    python -m AlphaGo.training.supervised_policy_trainer my_model.json debug_feature_planes.hdf5 training_results/ --epochs 5 --minibatch 32 --learning-rate 0.01

### Phase 2: reinforcement learning, self-play

Reinforcement learning of a policy is difficult to do from scratch. The supervised learning phase is used to _initialize_ the reinforcement learning phase, since greedily evaluating the policy from the first phase should yield reasonable moves.

The policy learned through supervised learning then plays against itself a few million times using decent-but-random moves. Every time it wins, its weights are adjusted to make selecting those same moves more likely. Every time it loses, its weights are adjusted in the opposite direction. Importantly, the network is not always playing against the best version of itself, but instead plays against a random past version of itself. This helps to ensure that it doesn't get stuck on a single type of strategy.

### Phase 3: reinforcement learning, value networks

The value network is trained to guess the outcome of games played by the policy we get from phase 2. To prevent overfitting (see the paper), this network is trained on _one board position per game_. Even more interestingly, there is an added element of randomness to reduce the network's bias (and possibly resulting in more creativity) at this phase; training data comes from games of self-play where, at a random point in the game, a completely random (off policy) move is made. The game is then continued past that point using the greedy policy. The value network is trained to predict the winner from the position immediately following the random move.

This network is simply trained to minimize squared error between output values and the result of the game (+1 for wins and -1 for losses).

### Phase 4: revisiting phase 2

Once the value network has learned to predict game outcomes, the self-play phase is repeated, except that instead of weights being adjusted based on wins and losses, they are adjusted based on _how different the outcome was from the value predicted by the value network_. Presumably this phase makes the policy and value functions more mutually consistent.