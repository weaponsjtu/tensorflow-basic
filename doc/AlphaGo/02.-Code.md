## General principles

We are using Python and emphasizing a modular/agile design so that this project stays as comprehensible as possible. The apparent lack of regard for speed should not worry you too much; see the section on [[benchmarking|Tests and Benchmarks]]

## Our implementation

__GameState__

We chose to create our own implementation of Go logic.This is handled by the `GameState` class in `AlphaGo/go.py`. It is nearly complete with a few minor bugs and the potential to be cleaned up or refactored. A `GameState` object is not meant to be much more than a represenatation of a single board position and the game logic for interacting with/updating it. These objects are the common currency that tie together the different components of the project. For example, we load SGF files into `GameState` objects before processing them into neural network features. A few optimizations have already been added that add a bit of bloat to the class, namely updating liberties on each move. Because some of the features AlphaGo uses depend on the game history, we also added `history` to the `GameState` class.

__MCTS__

A generic tree search class can be found in `AlphaGo/mcts.py` on the `mcts` branch. It has been made generic (i.e. not specific to Go or AlphaGo) so as to be easily understood, and to be easily testable in the absence of functional policy and value networks (which are still being written and will then need to be trained). There are many opportunities for benchmarking and optimizations of this file.

__Neural Networks__

For analogous reasons to MCTS, the policy network (see `CNNPolicy` in `AlphaGo/models/policy.py`) has been written "generically." In general, a policy is any function that maps from states to a distribution over actions. `CNNPolicy` is one such implementation that uses convolutional neural networks. Its `eval_state` function uses the feature `preprocessor` to convert from a `state` to neural network features, feeds them through a network, and converts the output to a list of `(action,probability)` tuples.

The value network is partially implemented. `policy.py` will serve as a template for `value.py`.

The rollout policy is not implemented yet, nor are any of the conversion functions for its features.

__Network Features__

In case you weren't yet sick of all the decoupling we've done already, we have separated out neural network _feature_ processing from the policy/value models themselves. The `Preprocessor` class in `AlphaGo/models/preprocessing.py` is responsible for knowing how to convert from a `GameState` instance to some network features.

There is some debate regarding how best to define the ladder features (discussion in issues).

Features for the rollout policy are not implemented yet.

__Training__

Training will be implemented as a series of scripts to be run from the command line. There are multiple stages of training that go into the full training 'pipeline'. The different stages of the training pipeline loosely correspond to our development milestones (with some complications glossed over in the main body of DeepMind's paper - see their Methods section)