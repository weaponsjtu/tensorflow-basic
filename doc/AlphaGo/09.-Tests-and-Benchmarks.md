## Unittests

We are using Python unittests to verify code. Not all existing code is tested, and not all existing tests are exhaustive. Writing new tests is a great way to become familiar with the project.

How to run all tests (starting in the root directory of the project)

	python -m unittest discover

How to run a specific test file (note: in order for this to work, the test file must contain `if __name__ == '__main__': unittest.main()`)

	python -m tests.test_policy

How to run a single function within a specific test file

	python -m tests.test_policy TestCNNPolicy.test_save_load

## Benchmarks

"Benchmarks" refer to speed and profiling scripts. These are not run automatically. These are written and run to identify critical speed bottlenecks.

Speed problems are not always where you would expect them to be. Because readability and simplicity are goals of this project, we are avoiding premature optimization and instead opting to rely heavily on benchmarking scripts to identify slow sections of code and - importantly - to quantify the value of a proposed speedup.

As an example, see `benchmarks/preprocessing_benchmark.py`. It uses the `cProfile` module to run a function and saves the profiling results in a `.prof` file. Run it as

    python -m benchmarks.preprocessing_benchmark

There are a number of tools for viewing and interpreting the results. See [snakeviz](https://jiffyclub.github.io/snakeviz) for example.

Writing new benchmarking scripts and/or addressing major speed issues is another great way to get started contributing.